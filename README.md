# LangGraph Agent Template

[![CI](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/unit-tests.yml)
[![Integration Tests](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/integration-tests.yml/badge.svg)](https://github.com/langchain-ai/new-langgraph-project/actions/workflows/integration-tests.yml)

A production-ready starter template for building LangGraph-based AI agents with LangGraph Server and visual debugging through LangGraph Studio.

## Architecture

![LangGraph Agent Template Architecture](./static/architecture.png)

<div align="center">
  <img src="./static/studio_ui.png" alt="Graph view in LangGraph studio UI" width="75%" />
</div>

## Overview

This template provides a minimal yet extensible foundation for building stateful AI agents using LangGraph. It includes:

- **Single-node graph**: A simple starting point (`src/agent/graph.py`) that processes state and applies runtime configuration
- **Type-safe state management**: Using dataclasses for robust state handling
- **Configurable parameters**: Dynamic configuration at runtime or assistant creation
- **Visual debugging**: Full integration with LangGraph Studio for graph visualization and step-by-step debugging
- **Production tooling**: Tests, linting, type checking, and dependency management with uv

## Getting Started

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) package manager

### Installation

1. Clone the repository and install dependencies using uv:

```bash
cd path/to/test-agent
uv sync
uv run pip install -e .
```

2. Install the LangGraph CLI for running the development server:

```bash
uv run pip install "langgraph-cli[inmem]"
```

3. (Optional) Set up environment variables for secrets and API keys:

```bash
cp .env.example .env
```

Configure your `.env` file with necessary API keys:

```text
# .env
LANGSMITH_API_KEY=lsv2...  # For LangSmith tracing (optional)
# Add other API keys as needed
```

### Running the Agent

Start the LangGraph development server:

```bash
uv run langgraph dev
```

The server will start on `http://localhost:8123` with LangGraph Studio available for visual debugging.

For production deployment information, see the [LangGraph Server documentation](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/).

## How to customize

1. **Define configurable parameters**: Modify the `Configuration` class in the `graph.py` file to expose the arguments you want to configure. For example, in a chatbot application you may want to define a dynamic system prompt or LLM to use. For more information on configurations in LangGraph, [see here](https://langchain-ai.github.io/langgraph/concepts/low_level/?h=configuration#configuration).

2. **Extend the graph**: The core logic of the application is defined in [graph.py](./src/agent/graph.py). You can modify this file to add new nodes, edges, or change the flow of information.

## Development

### Running Tests

```bash
# Run all tests
uv run pytest

# Run unit tests only
uv run pytest tests/unit_tests/

# Run integration tests
uv run pytest tests/integration_tests/
```

### Code Quality

```bash
# Run linting
uv run ruff check .

# Auto-fix linting issues
uv run ruff check --fix .

# Type checking
uv run mypy src/
```

### Working with LangGraph Studio

- **Hot reload**: Edit your graph code and changes are automatically applied
- **State debugging**: Edit past state and rerun from any point to debug specific nodes
- **Thread management**: Use the `+` button to create new threads with fresh state
- **Tracing**: Integrated with [LangSmith](https://smith.langchain.com/) for detailed execution traces

### Extending the Graph

The agent's core logic is in `src/agent/graph.py`. Common extensions include:

1. **Adding new nodes**: Create additional processing steps in your graph
2. **Implementing tools**: Connect external APIs or functions as tools
3. **State persistence**: Add memory or conversation history
4. **Complex routing**: Implement conditional edges based on state
5. **Streaming**: Enable token-by-token streaming for LLM responses

For advanced patterns and examples, see the [LangGraph documentation](https://langchain-ai.github.io/langgraph/).

<!--
Configuration auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
{
  "config_schemas": {
    "agent": {
      "type": "object",
      "properties": {}
    }
  }
}
-->
